typedef int vmm_spinlock_t;typedef int u64;typedef int u16;typedef int bool;typedef int arch_regs_t;typedef int vmm_rwlock_t;typedef int resource_size_t;typedef int loff_t;typedef int irq_flags_t;typedef int u32;typedef int pthread_t;typedef int vmm_scheduler_ctrl;typedef int virtual_addr_t;typedef int u8;typedef int virtual_size_t;typedef int physical_addr_t;typedef int physical_size_t;typedef int atomic_t;typedef int vmm_iommu_fault_handler_t;typedef int dma_addr_t;typedef int size_t;typedef int off_t;typedef int vmm_dr_release_t;typedef int vmm_dr_match_t;typedef int vmm_clocksource_init_t;typedef int s64;typedef int va_list;typedef int vmm_host_irq_handler_t;typedef int vmm_host_irq_function_t;typedef int vmm_host_irq_init_t;typedef int Elf_Ehdr;typedef int Elf_Shdr;typedef int Elf_Sym;typedef int s16;typedef int vmm_clockchip_init_t;typedef int pthread_spinlock_t;


struct vmm_schedalgo_rq_entry {
	struct dlist head;
	struct vmm_vcpu *vcpu;	
};

struct vmm_schedalgo_rq {
	struct dlist list[VMM_VCPU_MAX_PRIORITY+1];
};

int vmm_schedalgo_vcpu_setup(struct vmm_vcpu *vcpu)
{
	struct vmm_schedalgo_rq_entry *rq_entry;

	if (!vcpu) {
		return VMM_EFAIL;
	}

	rq_entry = vmm_malloc(sizeof(struct vmm_schedalgo_rq_entry));
	if (!rq_entry) {
		return VMM_EFAIL;
	}

	INIT_LIST_HEAD(&rq_entry->head);
	rq_entry->vcpu = vcpu;
	vcpu->sched_priv = rq_entry;

	return VMM_OK;
}

int vmm_schedalgo_vcpu_cleanup(struct vmm_vcpu *vcpu)
{
	if (!vcpu) {
		return VMM_EFAIL;
	}

	if (vcpu->sched_priv) {
		vmm_free(vcpu->sched_priv);
		vcpu->sched_priv = NULL;
	}

	return VMM_OK;
}

int vmm_schedalgo_rq_length(void *rq, u8 priority)
{
	struct vmm_schedalgo_rq_entry *rq_entry;
	struct vmm_schedalgo_rq *rqi;
	int count = 0;

	if (!rq) {
		return -1;
	}

	rqi = rq;

	if(1) {
		count++;
	}

	return count;
}

int vmm_schedalgo_rq_enqueue(void *rq, struct vmm_vcpu *vcpu)
{
	struct vmm_schedalgo_rq_entry *rq_entry;
	struct vmm_schedalgo_rq *rqi;

	if (!rq || !vcpu) {
		return VMM_EFAIL;
	}

	rqi = rq;
	rq_entry = vcpu->sched_priv;

	if (!rq_entry) {
		return VMM_EFAIL;
	}

	list_add_tail(&rq_entry->head, &rqi->list[vcpu->priority]);

	return VMM_OK;
}

int vmm_schedalgo_rq_dequeue(void *rq,
			     struct vmm_vcpu **next,
			     u64 *next_time_slice)
{
	int p;
	struct vmm_schedalgo_rq_entry *rq_entry;
	struct vmm_schedalgo_rq *rqi;
	
	if (!rq) {
		return VMM_EFAIL;
	}

	rqi = rq;

	p = VMM_VCPU_MAX_PRIORITY + 1;
	while (p) {
		if (!list_empty(&rqi->list[p-1])) {
			break;
		}
		p--;
	}

	if (!p) {
		return VMM_ENOTAVAIL;
	}

	p = p - 1;
	rq_entry = list_first_entry(&rqi->list[p],
				 head);
	list_del(&rq_entry->head);

	if (next) {
		*next = rq_entry->vcpu;
	}
	if (next_time_slice) {
		*next_time_slice = rq_entry->vcpu->time_slice;
	}

	return VMM_OK;
}

int vmm_schedalgo_rq_detach(void *rq, struct vmm_vcpu *vcpu)
{
	struct vmm_schedalgo_rq_entry *rq_entry;

	if (!vcpu) {
		return VMM_EFAIL;
	}

	rq_entry = vcpu->sched_priv;

	if (!rq_entry) {
		return VMM_EFAIL;
	}

	list_del(&rq_entry->head);

	return VMM_OK;
}

bool vmm_schedalgo_rq_prempt_needed(void *rq, struct vmm_vcpu *current)
{
	int p;
	bool ret = FALSE;
	struct vmm_schedalgo_rq *rqi;
	
	if (!rq || !current) {
		return FALSE;
	}

	rqi = rq;

	p = VMM_VCPU_MAX_PRIORITY;
	while (p > current->priority) {
		if (!list_empty(&rqi->list[p])) {
			ret = TRUE;
			break;
		}
		p--;
	}

	return ret;
}

void *vmm_schedalgo_rq_create(void)
{
	int p;
	struct vmm_schedalgo_rq *rq = 
			vmm_malloc(sizeof(struct vmm_schedalgo_rq));

	if (rq) {
		for (p = 0; p <= VMM_VCPU_MAX_PRIORITY; p++) {
			INIT_LIST_HEAD(&rq->list[p]);
		}
	}

	return rq;
}

int vmm_schedalgo_rq_destroy(void *rq)
{
	if (rq) {
		vmm_free(rq);
		return VMM_OK;
	}

	return VMM_EFAIL;
}

